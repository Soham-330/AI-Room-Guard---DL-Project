{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Hl6GrR_OSQrc",
        "qetfbrddYCoT",
        "PUJ72v9VaFZD"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 1"
      ],
      "metadata": {
        "id": "IzAM6zGeQvqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ›¡ï¸ Milestone 1: Activation and Basic Input\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "The goal of Milestone 1 was to implement **speech-based activation** for the AI Guard Agent.  \n",
        "Specifically:\n",
        "- Detect a **spoken command** (e.g., *â€œGuard my roomâ€*) from an audio file.  \n",
        "- Convert the audio into text using **Automatic Speech Recognition (ASR)**.  \n",
        "- Implement **state management** to switch Guard Mode ON/OFF.  \n",
        "- Provide **feedback** (console + optional Text-to-Speech).  \n",
        "\n",
        "---\n",
        "\n",
        "### âœ… What Was Done\n",
        "1. **Audio Input**  \n",
        "   - Since Colab cannot directly access the microphone, we used **uploaded audio recordings** (`.m4a`, `.wav`, `.mp3`).  \n",
        "   - Used `pydub` + `ffmpeg` to convert any format into a proper PCM `.wav` file.  \n",
        "\n",
        "2. **Speech Recognition**  \n",
        "   - Used `speech_recognition` (Google Web Speech API) to transcribe the uploaded command.  \n",
        "\n",
        "3. **Command Detection**  \n",
        "   - Implemented a simple logic to check if the command contains *â€œguard my roomâ€*.  \n",
        "   - If detected â†’ activate Guard Mode (`True`).  \n",
        "   - Otherwise â†’ remain inactive.  \n",
        "\n",
        "4. **Feedback**  \n",
        "   - Console message: ðŸ”’ *Guard Mode Activated!*  \n",
        "   - Added optional **Text-to-Speech (gTTS)** to give audio confirmation.  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“œ Code Implementation\n",
        "\n",
        "```python\n",
        "# Install dependencies\n",
        "!pip install speechrecognition pydub gtts\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# --- Step 1: Upload audio file ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # Upload your command.m4a / command.wav / command.mp3\n",
        "\n",
        "# --- Step 2: Convert audio to PCM WAV ---\n",
        "from pydub import AudioSegment\n",
        "\n",
        "input_file = list(uploaded.keys())[0]       # get uploaded filename\n",
        "output_file = \"command_fixed.wav\"\n",
        "\n",
        "sound = AudioSegment.from_file(input_file)  # auto-detects format\n",
        "sound = sound.set_channels(1).set_frame_rate(16000)  # mono + 16kHz\n",
        "sound.export(output_file, format=\"wav\")\n",
        "\n",
        "# --- Step 3: Speech Recognition ---\n",
        "import speech_recognition as sr\n",
        "\n",
        "recognizer = sr.Recognizer()\n",
        "with sr.AudioFile(output_file) as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    command = recognizer.recognize_google(audio_data)\n",
        "    print(\"You said:\", command)\n",
        "\n",
        "# --- Step 4: Guard Mode Activation Logic ---\n",
        "guard_mode = False\n",
        "\n",
        "if \"guard my room\" in command.lower():\n",
        "    guard_mode = True\n",
        "    print(\"ðŸ”’ Guard Mode Activated!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Command not recognized.\")\n",
        "\n",
        "print(\"Guard mode status:\", guard_mode)\n",
        "\n",
        "# --- Step 5: Optional Feedback with TTS ---\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "if guard_mode:\n",
        "    tts = gTTS(\"Guard mode activated\", lang='en')\n",
        "    tts.save(\"response.mp3\")\n",
        "    os.system(\"mpg123 response.mp3\")  # or: !apt-get install -y mpg123\n"
      ],
      "metadata": {
        "id": "1R8dyycnRMlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZZgJycWOgAC"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition pydub gtts playsound\n",
        "!pip install pydub\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # a popup will appear, select command.wav"
      ],
      "metadata": {
        "id": "dPzRF1j8Omdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Replace with your uploaded file name\n",
        "input_file = \"command.m4a\"\n",
        "output_file = \"command_fixed.wav\"\n",
        "\n",
        "sound = AudioSegment.from_file(input_file, format=\"m4a\")\n",
        "sound = sound.set_channels(1).set_frame_rate(16000)  # mono + 16kHz\n",
        "sound.export(output_file, format=\"wav\")\n"
      ],
      "metadata": {
        "id": "VACX2UONPya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "recognizer = sr.Recognizer()\n",
        "with sr.AudioFile(\"command_fixed.wav\") as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    command = recognizer.recognize_google(audio_data)\n",
        "    print(\"You said:\", command)\n"
      ],
      "metadata": {
        "id": "1uviHna7O-HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = command.lower()\n",
        "\n",
        "if \"guard my room\" in text:\n",
        "    if \"don't guard my room\" in text or \"do not guard my room\" in text or \"stop guarding\" in text:\n",
        "        guard_mode = False\n",
        "        print(\"ðŸ›‘ Guard Mode Deactivated!\")\n",
        "    else:\n",
        "        guard_mode = True\n",
        "        print(\"ðŸ”’ Guard Mode Activated!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Command not recognized.\")\n",
        "\n",
        "print(\"Guard mode status:\", guard_mode)\n"
      ],
      "metadata": {
        "id": "5VjBv0IlPDX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "if guard_mode:\n",
        "    tts = gTTS(\"Guard mode activated\", lang='en')\n",
        "    tts.save(\"response.mp3\")\n",
        "    os.system(\"mpg123 response.mp3\")  # or playsound(\"response.mp3\")\n"
      ],
      "metadata": {
        "id": "bGFVvcndPFjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 2"
      ],
      "metadata": {
        "id": "Hl6GrR_OSQrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python pillow\n",
        "!pip install gtts playsound"
      ],
      "metadata": {
        "id": "SNeKUjixZYR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install DeepFace"
      ],
      "metadata": {
        "id": "jv2fAZcloWNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative implementation using MediaPipe (more reliable in Colab)\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize MediaPipe\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Global storage\n",
        "known_face_features = []\n",
        "known_face_names = []"
      ],
      "metadata": {
        "id": "wIaKb_OTXkUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Enrollment"
      ],
      "metadata": {
        "id": "qetfbrddYCoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepface import DeepFace\n",
        "\n",
        "def extract_face_features_deepface(image):\n",
        "    \"\"\"\n",
        "    Extract face embeddings using DeepFace (Facenet model).\n",
        "    Returns 128-d or 512-d embedding and bounding box if available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # DeepFace automatically detects and aligns face internally\n",
        "        reps = DeepFace.represent(image, model_name='Facenet', enforce_detection=False)\n",
        "        if len(reps) > 0:\n",
        "            embedding = np.array(reps[0]['embedding'], dtype=np.float32)\n",
        "            region = reps[0]['facial_area']\n",
        "            bbox = (region['x'], region['y'], region['w'], region['h'])\n",
        "            return embedding, bbox\n",
        "        else:\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(\"âŒ DeepFace failed to extract features:\", e)\n",
        "        return None, None\n",
        "# ============ ENROLLMENT USING DEEPFACE ============\n",
        "\n",
        "known_face_features = []\n",
        "known_face_names = []\n",
        "\n",
        "def enroll_trusted_face_deepface(name):\n",
        "    \"\"\"\n",
        "    Enroll using DeepFace embeddings.\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ“¸ Please upload an image of {name}\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"Processing image: {filename}\")\n",
        "        image = cv2.imread(filename)\n",
        "        if image is None:\n",
        "            print(f\"âŒ Could not load image: {filename}\")\n",
        "            continue\n",
        "\n",
        "        embedding, bbox = extract_face_features_deepface(image)\n",
        "        if embedding is not None:\n",
        "            known_face_features.append(embedding)\n",
        "            known_face_names.append(name)\n",
        "\n",
        "            # Draw bbox if detected\n",
        "            if bbox:\n",
        "                x, y, w, h = bbox\n",
        "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                cv2.putText(image, f\"Enrolled: {name}\", (x, y-10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "            plt.figure(figsize=(8,6))\n",
        "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(f\"âœ… Enrolled {name}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"âŒ No face detected for {name}\")\n",
        "\n",
        "    return len(known_face_names)\n",
        "\n",
        "\n",
        "def save_encodings_deepface():\n",
        "    \"\"\"\n",
        "    Save DeepFace embeddings to pickle.\n",
        "    \"\"\"\n",
        "    if len(known_face_features) == 0:\n",
        "        print(\"âŒ No faces to save!\")\n",
        "        return\n",
        "\n",
        "    data = {\n",
        "        'features': known_face_features,\n",
        "        'names': known_face_names,\n",
        "        'method': 'deepface'\n",
        "    }\n",
        "\n",
        "    with open('face_features_deepface.pkl', 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    print(f\"âœ… Saved {len(known_face_names)} faces: {known_face_names}\")\n",
        "def enroll_trusted_face_cpu_only(name):\n",
        "    \"\"\"\n",
        "    CPU-only face_recognition enrollment\n",
        "    \"\"\"\n",
        "    # Force CPU mode\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "    # Restart face_recognition in CPU mode\n",
        "    try:\n",
        "        import face_recognition\n",
        "        face_recognition._load_dlib()  # Force reload\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(f\"Please upload an image of {name}\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"Processing image: {filename}\")\n",
        "\n",
        "        # Load image using OpenCV\n",
        "        image_bgr = cv2.imread(filename)\n",
        "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the uploaded image first\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(image_rgb)\n",
        "        plt.title(f\"Processing: {name}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        try:\n",
        "            # Force CPU-only face encoding with smaller model\n",
        "            face_encodings = face_recognition.face_encodings(\n",
        "                image_rgb,\n",
        "                num_jitters=1,  # Reduce jitters for speed\n",
        "                model='small'   # Use small model\n",
        "            )\n",
        "\n",
        "            if len(face_encodings) > 0:\n",
        "                face_encoding = face_encodings[0]\n",
        "                known_face_encodings.append(face_encoding)\n",
        "                known_face_names.append(name)\n",
        "\n",
        "                print(f\"âœ… Successfully enrolled {name} using CPU-only mode\")\n",
        "\n",
        "            else:\n",
        "                print(f\"âŒ No face found in the uploaded image for {name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ CPU-only encoding failed: {e}\")\n",
        "            print(\"Falling back to MediaPipe method...\")\n",
        "            return enroll_trusted_face_mediapipe(name)\n",
        "\n",
        "    return len(known_face_encodings)"
      ],
      "metadata": {
        "id": "YihgGEpvX04t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORKING ENROLLMENT - Choose one method\n",
        "print(\"=== FACE ENROLLMENT PROCESS (FIXED) ===\")\n",
        "\n",
        "# Method 1: MediaPipe (Most stable for Colab)\n",
        "print(\"Using MediaPipe for face detection...\")\n",
        "enroll_trusted_face_deepface(\"Soham\")\n",
        "save_encodings_deepface()\n",
        "\n",
        "# Method 2: CPU-only face_recognition (uncomment if you prefer)\n",
        "# print(\"Using CPU-only face_recognition...\")\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "# enroll_trusted_face_cpu_only(\"Soham\")\n",
        "# save_encodings()\n",
        "\n",
        "print(f\"Total trusted faces enrolled: {len(known_face_names)}\")\n"
      ],
      "metadata": {
        "id": "vsWZddbyX2Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Face Recognition"
      ],
      "metadata": {
        "id": "PUJ72v9VaFZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import files\n",
        "from base64 import b64decode\n",
        "from IPython.display import display, Javascript\n",
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "Z2q_ltYaadPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ RECOGNITION USING DEEPFACE ============\n",
        "\n",
        "# Load saved DeepFace embeddings\n",
        "with open('face_features_deepface.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "known_face_features = data['features']\n",
        "known_face_names = data['names']\n",
        "\n",
        "def recognize_faces_deepface(image, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Recognize faces in an image using DeepFace embeddings.\n",
        "    Returns list of (name, confidence, bbox).\n",
        "    \"\"\"\n",
        "    embedding, bbox = extract_face_features_deepface(image)\n",
        "    if embedding is None:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    best_name, best_sim = \"Unknown\", 0.0\n",
        "\n",
        "    for idx, known in enumerate(known_face_features):\n",
        "        sim = cosine_similarity([embedding], [known])[0][0]\n",
        "        if sim > threshold and sim > best_sim:\n",
        "            best_sim = sim\n",
        "            best_name = known_face_names[idx]\n",
        "\n",
        "    results.append((best_name, best_sim, bbox))\n",
        "    return results\n",
        "\n",
        "\n",
        "def display_recognition_deepface(image, results):\n",
        "    img = image.copy()\n",
        "    for name, conf, bbox in results:\n",
        "        if bbox:\n",
        "            x, y, w, h = bbox\n",
        "            color = (0,255,0) if name!=\"Unknown\" else (0,0,255)\n",
        "            cv2.rectangle(img, (x,y), (x+w,y+h), color, 2)\n",
        "            label = f\"{name} ({conf:.2f})\" if name!=\"Unknown\" else \"Unknown\"\n",
        "            cv2.putText(img, label, (x, y-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZjFvsm37aG2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Process a single uploaded image\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded:\n",
        "    img = cv2.imread(fn)\n",
        "    res = recognize_faces_deepface(img)\n",
        "    display_recognition_deepface(img, res)"
      ],
      "metadata": {
        "id": "t8LFc5pMaHfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Process an uploaded video file, annotate frames, save and display output video with bbox checks\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "import mediapipe as mp\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load saved MediaPipe features\n",
        "with open('face_features_deepface.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "known_face_features = data['features']\n",
        "known_face_names = data['names']\n",
        "\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "\n",
        "def extract_face_features_mediapipe(image):\n",
        "    \"\"\"\n",
        "    Extract face features using MediaPipe with bbox validity checks.\n",
        "    Returns feature vector and bounding box or (None, None).\n",
        "    \"\"\"\n",
        "    with mp_face_detection.FaceDetection(model_selection=1,\n",
        "                                          min_detection_confidence=0.5) as face_detection:\n",
        "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = face_detection.process(rgb)\n",
        "        if not results.detections:\n",
        "            return None, None\n",
        "        det = results.detections[0].location_data.relative_bounding_box\n",
        "        h, w, _ = rgb.shape\n",
        "        x, y = int(det.xmin * w), int(det.ymin * h)\n",
        "        w_box, h_box = int(det.width * w), int(det.height * h)\n",
        "        # Ensure bbox is within image\n",
        "        x, y = max(0, x), max(0, y)\n",
        "        w_box, h_box = min(w_box, w - x), min(h_box, h - y)\n",
        "        if w_box <= 0 or h_box <= 0:\n",
        "            return None, None\n",
        "        face = rgb[y:y + h_box, x:x + w_box]\n",
        "        if face.size == 0:\n",
        "            return None, None\n",
        "        face128 = cv2.resize(face, (128, 128)).flatten().astype(np.float32)\n",
        "        return face128, (x, y, w_box, h_box)\n",
        "\n",
        "def recognize_faces_mediapipe(image):\n",
        "    \"\"\"\n",
        "    Recognize faces in a BGR image.\n",
        "    Returns list of (name, confidence, bbox).\n",
        "    \"\"\"\n",
        "    feats, bbox = extract_face_features_mediapipe(image)\n",
        "    if feats is None:\n",
        "        return []\n",
        "    results = []\n",
        "    for idx, known in enumerate(known_face_features):\n",
        "        sim = cosine_similarity([feats], [known])[0][0]\n",
        "        if sim > 0.75:\n",
        "            results.append((known_face_names[idx], sim, bbox))\n",
        "            break\n",
        "    if not results:\n",
        "        results.append((\"Unknown\", 0.0, bbox))\n",
        "    return results\n",
        "\n",
        "def test_video_file_with_display_and_save_deepface():\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        cap = cv2.VideoCapture(fn)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"âŒ Cannot open video {fn}\")\n",
        "            continue\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
        "        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_name = f\"annotated_{fn}\"\n",
        "        out = cv2.VideoWriter(out_name, fourcc, fps, (w, h))\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            results = recognize_faces_deepface(frame)\n",
        "            for name, conf, bbox in results:\n",
        "                if bbox:\n",
        "                    x, y, bw, bh = bbox\n",
        "                    color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
        "                    cv2.rectangle(frame, (x, y), (x + bw, y + bh), color, 2)\n",
        "                    label = name if name != \"Unknown\" else \"Unknown\"\n",
        "                    cv2.putText(frame, label, (x, y - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "            out.write(frame)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"âœ… Saved annotated video as {out_name}\")\n",
        "\n",
        "        # Display preview\n",
        "        cap2 = cv2.VideoCapture(out_name)\n",
        "        ret2, frame2 = cap2.read()\n",
        "        if ret2:\n",
        "            cv2_imshow(frame2)\n",
        "        cap2.release()\n",
        "\n",
        "# Uncomment to run\n",
        "test_video_file_with_display_and_save_deepface()\n"
      ],
      "metadata": {
        "id": "GaPiVVwBceAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  AI Room Guard â€“ DeepFace Based Intruder Detection\n",
        "\n",
        "## ðŸ” Overview\n",
        "This project uses **DeepFace (Facenet)** and **OpenCV** to create an AI-powered guard system that detects and differentiates between **known** and **unknown** faces in a video.\n",
        "\n",
        "It can:\n",
        "- Enroll trusted faces.\n",
        "- Recognize them in uploaded videos.\n",
        "- Mark known persons in ðŸŸ© **green** and unknown persons in ðŸŸ¥ **red**.\n",
        "- Save the annotated video for review.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ How It Works\n",
        "1. **Enrollment Phase**\n",
        "   - Upload an image of a trusted person.\n",
        "   - DeepFace extracts embeddings (face features) and stores them in a pickle file.\n",
        "   ```python\n",
        "   enroll_trusted_face_deepface(\"PersonName\")\n",
        "   save_encodings_deepface()\n",
        "   ```\n",
        "2. **Recognition Phase**\n",
        "- Upload a video for analysis.\n",
        "- Each frame is processed to recognize known faces.\n",
        "- Annotated output is saved automatically.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nw9MXOeqGEI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 3"
      ],
      "metadata": {
        "id": "QG7kvJIwfU8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  AI Room Guard â€“ Milestone 3 (LLM + Timed Escalation + Email Alert)\n",
        "\n",
        "## ðŸš¨ Overview\n",
        "This system is an **intelligent AI Room Guard** that:\n",
        "- Detects and identifies faces using **DeepFace (Facenet)**  \n",
        "- Recognizes **known** vs. **unknown** persons  \n",
        "- Uses **Google Gemini LLM** to generate human-like escalation messages  \n",
        "- Converts text warnings to **speech (TTS)**  \n",
        "- Sends an **email alert** to the owner when the unknown person persists  \n",
        "\n",
        "All actions and warnings are **annotated on the video** in real time.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Main Features\n",
        "1. **Face Enrollment**  \n",
        "   Upload and register trusted faces (DeepFace embeddings are stored).  \n",
        "\n",
        "2. **Guard Mode**  \n",
        "   Upload a video â€” the system monitors each frame:  \n",
        "   - âœ… Known person â†’ green box with name  \n",
        "   - âŒ Unknown person â†’ red box + escalation messages  \n",
        "\n",
        "3. **Timed Escalation**\n",
        "   - Level 1: Gentle warning (spoken + on-screen)  \n",
        "   - Level 2: Stronger warning after 5 s  \n",
        "   - Level 3: Final alert, sends email to owner  \n",
        "\n",
        "4. **Video Annotation**\n",
        "   - All warnings appear as white text on the frame  \n",
        "   - Final video saved at:\n",
        "     ```\n",
        "     /content/Files/guard_out.mp4\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© How to Run\n",
        "1. **Set credentials** in Colab â†’ `userdata`:\n",
        "   ```python\n",
        "   userdata.set('SENDER_EMAIL', 'your@gmail.com')\n",
        "   userdata.set('SENDER_PASSWORD', 'app_password')\n",
        "   userdata.set('OWNER_EMAIL', 'owner@gmail.com')\n",
        "   userdata.set('API_KEY', 'your_gemini_api_key')\n",
        "```\n",
        "2. **Enroll Trusted Faces** :\n",
        "  ```python\n",
        "  feat, name = enroll_face()\n",
        "  save_faces([feat], [name])\n",
        "```\n",
        "\n",
        "3. **Activate Guard Mode**:  \n",
        "- Using Voice command\n",
        "\n",
        "4. **Downloading the Outputs**\n",
        "5. **The model integrates DeepFace, LLM, Speech, and Email â€” forming a complete smart surveillance assistant.**"
      ],
      "metadata": {
        "id": "EeBmxX21JhL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Milestone 3 with LLM + Timed Escalation + Email Alert\n",
        "# ================================\n",
        "\n",
        "# !pip install speechrecognition pydub gtts mediapipe opencv-python playsound openai\n",
        "# !apt-get install -y ffmpeg\n",
        "\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import pickle\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import google.generativeai as genai # For LLM\n",
        "from google.colab import userdata\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration / Credentials\n",
        "# ----------------------------\n",
        "\n",
        "FILES_DIR = \"Files\"\n",
        "os.makedirs(FILES_DIR, exist_ok=True)\n",
        "\n",
        "# OpenAI API key: set as environment variable or directly\n",
        "\n",
        "# Email config\n",
        "SMTP_SERVER = \"smtp.gmail.com\"\n",
        "SMTP_PORT = 587\n",
        "SENDER_EMAIL     = userdata.get('SENDER_EMAIL')\n",
        "SENDER_PASSWORD  = userdata.get('SENDER_PASSWORD')\n",
        "OWNER_EMAIL      = userdata.get('OWNER_EMAIL')\n",
        "\n",
        "# Similarity threshold for recognizing a known face\n",
        "SIM_THRESH = 0.75\n",
        "\n",
        "# Time thresholds (in seconds) for escalation levels\n",
        "ESCALATION_DELAY = 5  # seconds between levels\n",
        "\n",
        "# ----------------------------\n",
        "# Utility: Email sending\n",
        "# ----------------------------\n",
        "\n",
        "def send_email_alert(subject, body, to_addr=OWNER_EMAIL):\n",
        "    \"\"\"Send an email alert to the owner.\"\"\"\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = SENDER_EMAIL\n",
        "    msg['To'] = to_addr\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(body, 'plain'))\n",
        "    try:\n",
        "        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
        "        server.starttls()\n",
        "        server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
        "        server.sendmail(SENDER_EMAIL, to_addr, msg.as_string())\n",
        "        server.quit()\n",
        "        print(\"âœ… Email alert sent.\")\n",
        "    except Exception as e:\n",
        "        print(\"âŒ Failed to send email:\", str(e))\n",
        "\n",
        "# ----------------------------\n",
        "# Utility: Text-to-Speech\n",
        "# ----------------------------\n",
        "\n",
        "def speak(text, filename=\"response.mp3\"):\n",
        "    \"\"\"Convert text to speech and play.\"\"\"\n",
        "    tts = gTTS(text, lang='en')\n",
        "    path = os.path.join(FILES_DIR, filename)\n",
        "    tts.save(path)\n",
        "    print(\"Guard:\", text)\n",
        "    os.system(f\"mpg123 {path} >/dev/null 2>&1\")\n",
        "\n",
        "# ----------------------------\n",
        "# Face Recognition / Enrollment\n",
        "# ----------------------------\n",
        "\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "def extract_face_features(img):\n",
        "    \"\"\"\n",
        "    Extract 128/512-D face embedding using DeepFace (Facenet model).\n",
        "    Returns embedding vector and bounding box.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reps = DeepFace.represent(img, model_name='Facenet', enforce_detection=False)\n",
        "        if len(reps) > 0:\n",
        "            embedding = np.array(reps[0]['embedding'], dtype=np.float32)\n",
        "            region = reps[0]['facial_area']\n",
        "            bbox = (region['x'], region['y'], region['w'], region['h'])\n",
        "            return embedding, bbox\n",
        "        else:\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(\"âŒ DeepFace failed:\", e)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def enroll_face():\n",
        "    \"\"\"Upload an image to enroll a trusted face (DeepFace embedding).\"\"\"\n",
        "    print(\"Upload an image of the trusted person:\")\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        img = cv2.imread(fn)\n",
        "        emb, bbox = extract_face_features(img)\n",
        "        if emb is not None:\n",
        "            name = input(\"Enter name for this face: \")\n",
        "            return emb, name\n",
        "        else:\n",
        "            print(f\"âŒ No face detected in {fn}\")\n",
        "    return None, None\n",
        "\n",
        "def save_faces(features, names, path=os.path.join(FILES_DIR, \"face_features.pkl\")):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump({'features': features, 'names': names}, f)\n",
        "    print(\"Saved face features.\")\n",
        "\n",
        "def load_faces(path=os.path.join(FILES_DIR, \"face_features.pkl\")):\n",
        "    if not os.path.exists(path):\n",
        "        return [], []\n",
        "    with open(path, 'rb') as f:\n",
        "        d = pickle.load(f)\n",
        "    return d['features'], d['names']\n",
        "\n",
        "# ----------------------------\n",
        "# LLM-based escalation message generation\n",
        "# ----------------------------\n",
        "\n",
        "def generate_escalation_message(level, context=None):\n",
        "    genai.configure(api_key=userdata.get(\"API_KEY\"))\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-pro\")  # Use valid model name\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a guard AI in a security system. \"\n",
        "        f\"An unknown person is present. This is escalation level {level}. \"\n",
        "        \"Write a short polite-but-firm message asking the person to leave, \"\n",
        "        \"and warning consequences if he stays.\\n\\n\"\n",
        "        \"Message:\"\n",
        "    )\n",
        "    if context:\n",
        "        prompt += f\"\\nContext: {context}\\n\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "# ----------------------------\n",
        "# Guard Mode Video Processing with Escalation\n",
        "# ----------------------------\n",
        "\n",
        "def guard_mode_video_with_escalation():\n",
        "    known_feats, known_names = load_faces()\n",
        "    if not known_names:\n",
        "        print(\"No trusted face enrolled. Enroll first.\")\n",
        "        return\n",
        "\n",
        "    print(\"Upload video for guard monitoring:\")\n",
        "    uploaded = files.upload()\n",
        "    vid_name = list(uploaded.keys())[0]\n",
        "    cap = cv2.VideoCapture(vid_name)\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
        "    out_path = os.path.join(FILES_DIR, \"guard_out.mp4\")\n",
        "    out = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n",
        "\n",
        "    # Track escalation state\n",
        "    escalation_start_time = None\n",
        "    current_level = 0\n",
        "    person_present = False\n",
        "    msg = \"\"  # for on-frame message text\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        feat, bbox = extract_face_features(frame)\n",
        "        recognized = False\n",
        "\n",
        "        if feat is not None:\n",
        "            # Compare with enrolled embeddings\n",
        "            for idx, known in enumerate(known_feats):\n",
        "                sim = cosine_similarity([feat], [known])[0][0]\n",
        "                if sim > 0.6:  # DeepFace similarity threshold\n",
        "                    recognized = True\n",
        "                    # ðŸŸ© Draw green box for known face\n",
        "                    x, y, bw, bh = bbox\n",
        "                    cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, known_names[idx], (x, y - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "                    break\n",
        "\n",
        "        if not recognized and feat is not None:\n",
        "            # ðŸŸ¥ Unknown person detected\n",
        "            person_present = True\n",
        "            x, y, bw, bh = bbox\n",
        "            cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 0, 255), 2)  # red box\n",
        "            cv2.putText(frame, \"Unknown\", (x, y - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "            # ðŸ”” Escalation logic\n",
        "            if escalation_start_time is None:\n",
        "                escalation_start_time = time.time()\n",
        "                current_level = 1\n",
        "                msg = generate_escalation_message(current_level)\n",
        "                speak(msg)\n",
        "            else:\n",
        "                elapsed = time.time() - escalation_start_time\n",
        "                if current_level == 1 and elapsed > ESCALATION_DELAY:\n",
        "                    current_level = 2\n",
        "                    msg = generate_escalation_message(current_level)\n",
        "                    speak(msg)\n",
        "                elif current_level == 2 and elapsed > 2 * ESCALATION_DELAY:\n",
        "                    current_level = 3\n",
        "                    msg = generate_escalation_message(current_level)\n",
        "                    speak(msg)\n",
        "\n",
        "                    # ðŸ’Œ Send final email alert\n",
        "                    subject = \"ðŸš¨ Security Alert: Unknown Person Detected\"\n",
        "                    body = f\"An unknown person remained after multiple warnings.\\n\\nLast message: {msg}\"\n",
        "                    send_email_alert(subject, body)\n",
        "                    print(\"ðŸ›‘ Level 3 escalation reached â€” alert sent.\")\n",
        "\n",
        "                    # Write final annotated frame before exit\n",
        "                    cv2.putText(frame, msg, (30, h - 40),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
        "                                (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                    out.write(frame)\n",
        "                    break  # exit loop after escalation Level 3\n",
        "\n",
        "            # ðŸ§¾ Annotate LLM message on frame\n",
        "            if msg:\n",
        "                cv2.putText(frame, msg, (30, h - 40),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
        "                            (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        else:\n",
        "            # If no unknowns, reset state\n",
        "            if person_present:\n",
        "                person_present = False\n",
        "                escalation_start_time = None\n",
        "                current_level = 0\n",
        "                msg = \"\"\n",
        "\n",
        "        # Write processed frame\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"ðŸ”’ Guard video processing done. Output saved at: {out_path}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Flow\n",
        "# ----------------------------\n",
        "\n",
        "def get_voice_command():\n",
        "    print(\"Upload audio file with activation command:\")\n",
        "    uploaded = files.upload()\n",
        "    fn = list(uploaded.keys())[0]\n",
        "    from pydub import AudioSegment\n",
        "    audio = AudioSegment.from_file(fn)\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    wav_path = os.path.join(FILES_DIR, \"cmd.wav\")\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "    r = sr.Recognizer()\n",
        "    with sr.AudioFile(wav_path) as src:\n",
        "        aud = r.record(src)\n",
        "        cmd = r.recognize_google(aud)\n",
        "        print(\"Command:\", cmd)\n",
        "        return cmd.lower()\n",
        "\n",
        "def process_command(cmd):\n",
        "    if \"guard my room\" in cmd and not (\"stop\" in cmd or \"don't\" in cmd):\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "Xkgz5dfha62M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    print(\"=== AI Guard with LLM & Escalation ===\")\n",
        "    cmd = get_voice_command()\n",
        "    if process_command(cmd):\n",
        "        speak(\"Guard mode activating. Please enroll a face.\")\n",
        "        feat, name = enroll_face()\n",
        "        if feat is None:\n",
        "            print(\"Enrollment failed. Exiting.\")\n",
        "        else:\n",
        "            save_faces([feat], [name])\n",
        "            speak(\"Enrollment done. Please upload video for monitoring.\")\n",
        "            guard_mode_video_with_escalation()\n",
        "    else:\n",
        "        speak(\"Guard mode not activated.\")\n"
      ],
      "metadata": {
        "id": "qik8EETDbGpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}